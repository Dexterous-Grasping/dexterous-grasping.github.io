<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>Dexterous Grasping  of Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div class="nav">
    <div class="nav-container">
      <img src="assets/iccv-navbar-logo.svg" alt="ICCV Logo" style="position: absolute; left: 1em; max-height: 40px;">
      <a href="#">Home</a>
      <a href="#intro">Introduction</a>
      <a href="#dataset">Benchmark Dataset</a>
      <a href="#call">Call for Papers</a>
    </div>
  </div>

  <div class="title-container">
    <div class="overlay"></div>
    <div class="content" style="text-align: center; margin: 20px">
      <h1>Dexterous Grasping  of Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025</h1>
      <div class="subtitle">
        <a href="https://iccv.thecvf.com/">ICCV 2025</a> Workshop
      </div>
      <div class="subtitle">Oct 20th (Afternoon), 2025</div>
      <div class="subtitle">Honolulu, Hawai'i</div>
    </div>
  </div>

  <div class="container">
    <div class="section" id="intro">
      <h2>Introduction</h2>

      <p>
        Dexterous Grasping is one of the key challenges in robotics, involving the ability of robotic hands to flexibly manipulate objects of various shapes, sizes, and materials. With the rapid advancement of intelligent robot technology, embodied agents are increasingly expected to work and live alongside humans in households, factories, hospitals, schools, and more. For these agents to operate safely, socially, and intelligently, <b>they must master dexterous grasping skills, effectively interact with humans, and adapt to changing environments</b>.
      </p>
      
      <p>
        As a dedicated sub-area within the Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025, we focus on advancing dexterous grasping technologies. Our scope encompasses, but is not limited to:

        <ul>
          <li>Transferring knowledge from human grasping and manipulation behaviors to different types of robotic hands (e.g., via retargeting for 2-finger, 3-finger, 4-finger and 5-finger robotic hands).</li>
          <li>Investigating designs, control strategies and learning algorithms for dexterous hands to achieve more natural, human-like grasping actions with biomimetic motion planning and force distribution.</li>
          <li>Developing adaptive grasping algorithms based on tactile and visual feedback to handle uncertainty and dynamic environments.</li>
          <li>Establishing meaningful benchmarks and metrics to measure advancements in dexterous grasping.</li>
        </ul>
      </p>
    </div>

    <div class="section" id="Benchmark Dataset">
      <h2>Benchmark Dataset</h2>
      <p>
        To promote fair comparison and evaluation of dexterous grasping algorithms, we provide a standardized benchmark dataset containing 15698 diverse objects with different shapes and sizes. Each object comes with detailed 3D models and grasping annotations. Our evaluation framework includes standardized metrics for assessing grasping stability success rates, collision detection, and the diversity of generated grasp poses. All participants are encouraged to use this dataset to facilitate objective comparison across different approaches. The dataset and evaluation tools are available through the download links below.
      </p>

      <p>
        While we recommend using this benchmark dataset for evaluation, participants are also welcome to validate their methods using other public or proprietary datasets. Regardless of the dataset used, all submissions should be supported by solid theoretical foundations and experimental results, with detailed descriptions of the datasets, evaluation methodologies, and experimental setups in the paper. Our goal is to encourage innovative research in the field of dexterous grasping, not limited to the use of specific datasets.
      </p>

      <h3>Resources</h3>
      <p>
        <ul>
          <li><a href="https://huggingface.co/datasets/GaussionZhong/DexGrasp-Anything/blob/main/Dexgraspanyting.tar.gz" target="_blank">Dataset (15GB)</a></li>
          <li><a href="https://github.com/4DVLab/DexGrasp-Anything/blob/main/datasets/Grasp_anyting.py" target="_blank">Dataset Loading Reference</a></li>
          <li><a href="https://github.com/4DVLab/DexGrasp-Anything/tree/main/envs/tasks" target="_blank">Evaluation Guidelines</a></li>
          <li><a href="https://dexgraspanything.github.io/" target="_blank">Reference Paper: DexGraspAnything: Towards Universal Robotic Dexterous Grasping with Physics Awareness</a></li>
        </ul>
      </p>
      <p><strong>Disclaimer:</strong> This dataset is provided for academic research purposes only and may not be used for commercial applications. By using this dataset, you agree to use it solely for non-commercial academic research.</p>

    <div class="section" id="call">
      <h2>Call for Papers</h2>
      <p>
        We invite submissions of <strong>long papers</strong> (up to 8 pages excluding references) and <strong>short papers</strong> (up to 4 pages excluding references) that explore dexterous grasping.
      </p>

      <h3>Paper Topics</h3>
      <p>
        Topics of interests include, but are not limited to:
        <ul>
          <li><strong>Dexterous Grasping Algorithms</strong>: Developing grasping planning and execution algorithms capable of handling various object shapes, materials, and poses.</li>
          <li><strong>Human Grasping Behavior Modeling</strong>: Advancing techniques for capturing and modeling human grasping actions and interactions.</li>
          <li><strong>Knowledge Transfer</strong>: Leveraging insights from human grasping behaviors to inform the development of dexterous robotic hands.</li>
          <li><strong>Cross-Hand Retargeting</strong>: Techniques for adapting grasping strategies across different hand morphologies (e.g., from human hands to various robotic hand configurations).</li>
          <li><strong>Tactile Sensing and Control</strong>: Exploring tactile feedback in dexterous grasping to achieve fine force control and object manipulation.</li>
          <li><strong>Benchmarking and Metrics</strong>: Establishing meaningful benchmarks and metrics to assess advancements in dexterous grasping. Especially for the human-like grasping.</li>
        </ul>
      </p>

      <h3>Submission Guidelines</h3>
      <p>As a dedicated sub-area within the Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025, please submit your papers to the <a href="https://human-robot-scene.github.io/" target="_blank">Human-Robot-Scene Interaction and Collaboration Workshop</a>.</p>
    </div>
  <footer>
    <p>&copy; Dexterous Grasping  of Human-Robot-Scene Interaction and Collaboration Workshop @ICCV2025</p>
  </footer>
</body>

</html>